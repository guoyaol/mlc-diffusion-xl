{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend.torch import dynamo_capture_subgraphs\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "from tvm.script import relax as R\n",
    "\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "from web_stable_diffusion import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guoyaoli/tvm_work/tvm/python/tvm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(tvm.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_text_embeddings(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            text_embeddings = self.clip(text_input_ids)[0]\n",
    "            return text_embeddings\n",
    "\n",
    "    clip = pipe.text_encoder\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_latents_to_noise_pred(pipe, device_str: str) -> tvm.IRModule:\n",
    "    class UNetModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, unet):\n",
    "            super().__init__()\n",
    "            self.unet = unet\n",
    "            # Default guidance scale factor in stable diffusion.\n",
    "            self.guidance_scale = 7.5\n",
    "\n",
    "        def forward(self, latents, timestep_tensor, text_embeddings, added_cond_kwargs_text_embeds, added_cond_kwargs_text_time_ids):\n",
    "            # Latent concatenation.\n",
    "            latent_model_input = torch.cat([latents] * 2, dim=0)\n",
    "            # UNet forward.\n",
    "            noise_pred = self.unet(latent_model_input, timestep_tensor, text_embeddings, added_cond_kwargs_text_embeds, added_cond_kwargs_text_time_ids)\n",
    "            # Classifier-free guidance.\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + self.guidance_scale * (\n",
    "                noise_pred_text - noise_pred_uncond\n",
    "            )\n",
    "            return noise_pred\n",
    "\n",
    "    unet = utils.get_unet(pipe, device_str)\n",
    "    unet_to_noise_pred = UNetModelWrapper(unet)\n",
    "    graph = fx.symbolic_trace(unet_to_noise_pred)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 128, 128), \"float32\"), ((), \"int32\"), ((2, 77, 2048), \"float32\"), \n",
    "         ((2, 1280), \"float32\"), ((2, 6), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"unet\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_to_image(pipe) -> tvm.IRModule:\n",
    "    class VAEModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            # Scale the latents so that it can be decoded by VAE.\n",
    "            latents = 1 / 0.18215 * latents\n",
    "            # VAE decode\n",
    "            z = self.vae.post_quant_conv(latents)\n",
    "            image = self.vae.decoder(z)\n",
    "            # Image normalization\n",
    "            image = (image / 2 + 0.5).clamp(min=0, max=1)\n",
    "            image = (image.permute(0, 2, 3, 1) * 255).round()\n",
    "            return image\n",
    "\n",
    "    vae = utils.get_vae(pipe)\n",
    "    vae_to_image = VAEModelWrapper(vae)\n",
    "\n",
    "    # z = torch.rand((1, 4, 64, 64), dtype=torch.float32)\n",
    "    # mod = dynamo_capture_subgraphs(\n",
    "    #     vae_to_image.forward,\n",
    "    #     z,\n",
    "    #     keep_params_as_input=True,\n",
    "    # )\n",
    "    # assert len(mod.functions) == 1\n",
    "\n",
    "    # return tvm.IRModule({\"vae\": mod[\"subgraph_0\"]})\n",
    "    graph = fx.symbolic_trace(vae_to_image)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 64, 64), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"vae\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_discrete_scheduler_steps() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # step, the function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    sigma = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "    sigma_next = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "\n",
    "    with bb.function(\n",
    "        \"euler_discrete_scheduler_step\",\n",
    "        [sample, model_output, sigma, sigma_next],\n",
    "    ):\n",
    "        prev_sample = bb.emit(\n",
    "            sample + model_output * (sigma_next - sigma),\n",
    "            \"prev_sample\",\n",
    "        )\n",
    "        bb.emit_func_output(prev_sample)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    }
   ],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "\n",
    "torch_dev_key = \"cpu\"\n",
    "\n",
    "clip = clip_to_text_embeddings(pipe)\n",
    "unet = unet_latents_to_noise_pred(pipe, torch_dev_key)\n",
    "vae = vae_to_image(pipe)\n",
    "# concat_embeddings = concat_embeddings()\n",
    "# image_to_rgba = image_to_rgba()\n",
    "scheduler = euler_discrete_scheduler_steps()\n",
    "\n",
    "mod: tvm.IRModule = utils.merge_irmodules(\n",
    "    clip,\n",
    "    unet,\n",
    "    vae,\n",
    "    # concat_embeddings,\n",
    "    # image_to_rgba,\n",
    "    scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
