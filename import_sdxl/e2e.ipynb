{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guoyaol/.conda/envs/sdxl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend.torch import dynamo_capture_subgraphs\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "from tvm.script import relax as R\n",
    "\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "from web_stable_diffusion import utils\n",
    "from web_stable_diffusion import trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/guoyaol/tvm/python/tvm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(tvm.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to your desired CUDA version\n",
    "cuda_path = \"/usr/local/cuda-12.1/bin\"\n",
    "lib_path = \"/usr/local/cuda-12.1/lib64\"\n",
    "\n",
    "# Set PATH\n",
    "os.environ['PATH'] = cuda_path + ':' + os.environ['PATH']\n",
    "\n",
    "# Set LD_LIBRARY_PATH\n",
    "if 'LD_LIBRARY_PATH' in os.environ:\n",
    "    os.environ['LD_LIBRARY_PATH'] = lib_path + ':' + os.environ['LD_LIBRARY_PATH']\n",
    "else:\n",
    "    os.environ['LD_LIBRARY_PATH'] = lib_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda-12.1/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "!which nvcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_text_embeddings(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            result = self.clip(text_input_ids, output_hidden_states=True)\n",
    "            text_embeddings = result.hidden_states[-2]\n",
    "            pool_text_embeddings = result[0]\n",
    "            return text_embeddings, pool_text_embeddings\n",
    "\n",
    "    clip = pipe.text_encoder\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_text_embeddings2(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            result = self.clip(text_input_ids, output_hidden_states=True)\n",
    "            text_embeddings = result.hidden_states[-2]\n",
    "            pool_text_embeddings = result.text_embeds\n",
    "            return text_embeddings, pool_text_embeddings\n",
    "\n",
    "    clip = utils.get_clip(pipe)\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip2\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_latents() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    latents = relax.Var(\"latents\", R.Tensor([1, 4, 128, 128], \"float32\"))\n",
    "\n",
    "    with bb.function(\"cat_latents\", [latents]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([latents, latents], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_latents_to_noise_pred(pipe, device_str: str) -> tvm.IRModule:\n",
    "    class UNetModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, unet):\n",
    "            super().__init__()\n",
    "            self.unet = unet\n",
    "            # Default guidance scale factor in stable diffusion.\n",
    "            self.guidance_scale = 5.0\n",
    "\n",
    "        def forward(self, latents, timestep_tensor, text_embeddings, added_cond_kwargs_text_embeds, added_cond_kwargs_text_time_ids):\n",
    "            # UNet forward.\n",
    "            noise_pred = self.unet(latents, timestep_tensor, text_embeddings, added_cond_kwargs_text_embeds, added_cond_kwargs_text_time_ids)\n",
    "            # Classifier-free guidance.\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + self.guidance_scale * (\n",
    "                noise_pred_text - noise_pred_uncond\n",
    "            )\n",
    "            return noise_pred\n",
    "\n",
    "    unet = utils.get_unet(pipe, device_str)\n",
    "    unet_to_noise_pred = UNetModelWrapper(unet)\n",
    "    graph = fx.symbolic_trace(unet_to_noise_pred)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((2, 4, 128, 128), \"float32\"), ((), \"int32\"), ((2, 77, 2048), \"float32\"), \n",
    "         ((2, 1280), \"float32\"), ((2, 6), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"unet\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_to_image(pipe) -> tvm.IRModule:\n",
    "    class VAEModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            # Scale the latents so that it can be decoded by VAE.\n",
    "            latents = 1 / 0.13025 * latents\n",
    "            # VAE decode\n",
    "            z = self.vae.post_quant_conv(latents)\n",
    "            image = self.vae.decoder(z)\n",
    "            # Image normalization\n",
    "            image = (image / 2 + 0.5).clamp(min=0, max=1)\n",
    "            image = (image.permute(0, 2, 3, 1) * 255).round()\n",
    "            return image\n",
    "\n",
    "    vae = utils.get_vae(pipe)\n",
    "    vae_to_image = VAEModelWrapper(vae)\n",
    "\n",
    "    # z = torch.rand((1, 4, 64, 64), dtype=torch.float32)\n",
    "    # mod = dynamo_capture_subgraphs(\n",
    "    #     vae_to_image.forward,\n",
    "    #     z,\n",
    "    #     keep_params_as_input=True,\n",
    "    # )\n",
    "    # assert len(mod.functions) == 1\n",
    "\n",
    "    # return tvm.IRModule({\"vae\": mod[\"subgraph_0\"]})\n",
    "    graph = fx.symbolic_trace(vae_to_image)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 128, 128), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"vae\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_embeddings() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 77, 2048], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 77, 2048], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_embeddings\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_enocder_outputs() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 77, 768], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 77, 1280], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_enocder_outputs\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=-1)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_pool_embeddings() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 1280], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 1280], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_pool_embeddings\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to rgba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_rgba() -> tvm.IRModule:\n",
    "    from tvm import te\n",
    "\n",
    "    def f_image_to_rgba(A):\n",
    "        def fcompute(y, x):\n",
    "            return (\n",
    "                A[0, y, x, 0].astype(\"uint32\")\n",
    "                | (A[0, y, x, 1].astype(\"uint32\") << 8)\n",
    "                | (A[0, y, x, 2].astype(\"uint32\") << 16)\n",
    "                | tvm.tir.const(255 << 24, \"uint32\")\n",
    "            )\n",
    "\n",
    "        return te.compute((1024, 1024), fcompute, name=\"image_to_rgba\")\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "    x = relax.Var(\"x\", R.Tensor([1, 1024, 1024, 3], \"float32\"))\n",
    "    with bb.function(\"image_to_rgba\", [x]):\n",
    "        image = bb.emit(\n",
    "            bb.call_te(f_image_to_rgba, x, primfunc_name_hint=\"tir_image_to_rgba\")\n",
    "        )\n",
    "        bb.emit_func_output(image)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_discrete_scheduler_steps() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # step, the function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 128, 128), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 128, 128), \"float32\"))\n",
    "    sigma = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "    sigma_next = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "\n",
    "    with bb.function(\n",
    "        \"euler_discrete_scheduler_step\",\n",
    "        [sample, model_output, sigma, sigma_next],\n",
    "    ):\n",
    "        prev_sample = bb.emit(\n",
    "            sample + model_output * (sigma_next - sigma),\n",
    "            \"prev_sample\",\n",
    "        )\n",
    "        bb.emit_func_output(prev_sample)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_discrete_scheduler_scale() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # scale, the function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((2, 4, 128, 128), \"float32\"))\n",
    "    sigma = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "\n",
    "    with bb.function(\n",
    "        \"euler_discrete_scheduler_scale\",\n",
    "        [sample, sigma],\n",
    "    ):\n",
    "        scaled_latent_model_input = bb.emit(\n",
    "            sample / ((sigma** relax.const(2.0) + relax.const(1.0)) ** relax.const(0.5)),\n",
    "            \"scaled_latent_model_input\",\n",
    "        )\n",
    "        bb.emit_func_output(scaled_latent_model_input)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'add_watermarker': None} were passed to StableDiffusionXLPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'add_watermarker': None} are not expected by StableDiffusionXLPipeline and will be ignored.\n",
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "_capture raised TVMError: Traceback (most recent call last):\n  [bt] (8) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::Normalize(tvm::RelayExpr const&)+0x113) [0x7f0f0fb229e3]\n  [bt] (7) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>*)#9}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>*)+0x2c) [0x7f0f0fb149ec]\n  [bt] (6) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::VisitExpr_(tvm::relax::CallNode const*)+0x563) [0x7f0f0fb25043]\n  [bt] (5) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::InferStructInfo(tvm::relax::Call const&)+0x2b2) [0x7f0f0fb24082]\n  [bt] (4) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::relax::StructInfo (tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>::AssignTypedLambda<tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>(tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)+0x13c) [0x7f0f0fc25c6c]\n  [bt] (3) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::InferStructInfoSqueeze(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)+0x441) [0x7f0f0fd51aa1]\n  [bt] (2) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::BlockBuilderImpl::ReportFatal(tvm::Diagnostic const&)+0x59) [0x7f0f0fb1de59]\n  [bt] (1) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x3d) [0x7f0f0f32668d]\n  [bt] (0) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::Backtrace[abi:cxx11]()+0x2c) [0x7f0f11bf169c]\n  File \"/home/guoyaol/tvm/src/relax/ir/block_builder.cc\", line 138\nTVMError: Squeeze expects the input tensor shape values at the given axis positions to be all 1. However, the tensor shape at axis 1 is T.int64(1280) which is not 1. If it is symbolic, please use MatchCast to cast it to 1 before doing Squeeze.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/tvm/python/tvm/relax/frontend/torch/dynamo.py:161\u001b[0m, in \u001b[0;36mdynamo_capture_subgraphs.<locals>._capture\u001b[0;34m(graph_module, example_inputs)\u001b[0m\n\u001b[1;32m    160\u001b[0m input_info \u001b[39m=\u001b[39m [(\u001b[39mtuple\u001b[39m(tensor\u001b[39m.\u001b[39mshape), \u001b[39mstr\u001b[39m(tensor\u001b[39m.\u001b[39mdtype)) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m example_inputs]\n\u001b[0;32m--> 161\u001b[0m mod_ \u001b[39m=\u001b[39m from_fx(\n\u001b[1;32m    162\u001b[0m     graph_module,\n\u001b[1;32m    163\u001b[0m     input_info,\n\u001b[1;32m    164\u001b[0m     keep_params_as_input\u001b[39m=\u001b[39;49mkeep_params_as_input,\n\u001b[1;32m    165\u001b[0m     unwrap_unit_return_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m new_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msubgraph_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(mod\u001b[39m.\u001b[39mget_global_vars())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/tvm/python/tvm/relax/frontend/torch/fx_translator.py:1593\u001b[0m, in \u001b[0;36mfrom_fx\u001b[0;34m(model, input_info, keep_params_as_input, unwrap_unit_return_tuple, no_bind_return_tuple)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert a PyTorch FX GraphModule to a Relax program\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \n\u001b[1;32m   1507\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[39mcheck the placeholder rows in the beginning of the tabular.\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1593\u001b[0m \u001b[39mreturn\u001b[39;00m TorchFXImporter()\u001b[39m.\u001b[39;49mfrom_fx(\n\u001b[1;32m   1594\u001b[0m     model, input_info, keep_params_as_input, unwrap_unit_return_tuple, no_bind_return_tuple\n\u001b[1;32m   1595\u001b[0m )\n",
      "File \u001b[0;32m~/tvm/python/tvm/relax/frontend/torch/fx_translator.py:1485\u001b[0m, in \u001b[0;36mTorchFXImporter.from_fx\u001b[0;34m(self, model, input_info, keep_params_as_input, unwrap_unit_return_tuple, no_bind_return_tuple)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m   1483\u001b[0m         node\u001b[39m.\u001b[39mtarget \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_map\n\u001b[1;32m   1484\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnsupported function target \u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mtarget\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1485\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv[node] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_map[node\u001b[39m.\u001b[39;49mtarget](node)\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/tvm/python/tvm/relax/frontend/torch/fx_translator.py:583\u001b[0m, in \u001b[0;36mTorchFXImporter._squeeze\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    582\u001b[0m     dim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_builder\u001b[39m.\u001b[39;49memit(relax\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49msqueeze(x, dim))\n",
      "File \u001b[0;32m~/tvm/python/tvm/relax/block_builder.py:307\u001b[0m, in \u001b[0;36mBlockBuilder.emit\u001b[0;34m(self, expr, name_hint)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Emit an expr.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39mThis infers the shape and type of the expr, create a variable,\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mand bind the expr to the variable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m    A newly created variable that gets bound to the input expr.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m \u001b[39mreturn\u001b[39;00m _ffi_api\u001b[39m.\u001b[39;49mBlockBuilderEmit(\u001b[39mself\u001b[39;49m, expr, name_hint)\n",
      "File \u001b[0;32m~/tvm/python/tvm/_ffi/_ctypes/packed_func.py:238\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    237\u001b[0m ):\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::Normalize(tvm::RelayExpr const&)+0x113) [0x7f0f0fb229e3]\n  [bt] (7) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>*)#9}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>*)+0x2c) [0x7f0f0fb149ec]\n  [bt] (6) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::VisitExpr_(tvm::relax::CallNode const*)+0x563) [0x7f0f0fb25043]\n  [bt] (5) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::InferStructInfo(tvm::relax::Call const&)+0x2b2) [0x7f0f0fb24082]\n  [bt] (4) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::relax::StructInfo (tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>::AssignTypedLambda<tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>(tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)+0x13c) [0x7f0f0fc25c6c]\n  [bt] (3) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::InferStructInfoSqueeze(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)+0x441) [0x7f0f0fd51aa1]\n  [bt] (2) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::BlockBuilderImpl::ReportFatal(tvm::Diagnostic const&)+0x59) [0x7f0f0fb1de59]\n  [bt] (1) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x3d) [0x7f0f0f32668d]\n  [bt] (0) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::Backtrace[abi:cxx11]()+0x2c) [0x7f0f11bf169c]\n  File \"/home/guoyaol/tvm/src/relax/ir/block_builder.cc\", line 138\nTVMError: Squeeze expects the input tensor shape values at the given axis positions to be all 1. However, the tensor shape at axis 1 is T.int64(1280) which is not 1. If it is symbolic, please use MatchCast to cast it to 1 before doing Squeeze.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m torch_dev_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m clip \u001b[39m=\u001b[39m clip_to_text_embeddings(pipe)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m clip2 \u001b[39m=\u001b[39m clip_to_text_embeddings2(pipe)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m unet \u001b[39m=\u001b[39m unet_latents_to_noise_pred(pipe, torch_dev_key)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m vae \u001b[39m=\u001b[39m vae_to_image(pipe)\n",
      "\u001b[1;32m/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m text_input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((\u001b[39m1\u001b[39m, \u001b[39m77\u001b[39m))\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mint32)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Capture CLIP's computational graph.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m mod \u001b[39m=\u001b[39m dynamo_capture_subgraphs(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     clip_to_text_embeddings\u001b[39m.\u001b[39;49mforward,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     text_input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     keep_params_as_input\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(mod\u001b[39m.\u001b[39mfunctions) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfleet/home/guoyaol/mlc-diffusion-refiner/import_sdxl/e2e.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tvm\u001b[39m.\u001b[39mIRModule({\u001b[39m\"\u001b[39m\u001b[39mclip2\u001b[39m\u001b[39m\"\u001b[39m: mod[\u001b[39m\"\u001b[39m\u001b[39msubgraph_0\u001b[39m\u001b[39m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/tvm/python/tvm/relax/frontend/torch/dynamo.py:175\u001b[0m, in \u001b[0;36mdynamo_capture_subgraphs\u001b[0;34m(model, *params, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m compiled_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcompile(model, backend\u001b[39m=\u001b[39m_capture)\n\u001b[1;32m    174\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 175\u001b[0m     compiled_model(\u001b[39m*\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1792\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1787\u001b[0m _step_logger()(\n\u001b[1;32m   1788\u001b[0m     logging\u001b[39m.\u001b[39mINFO,\n\u001b[1;32m   1789\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo done tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m (RETURN_VALUE)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m )\n\u001b[1;32m   1791\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE triggered compile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1792\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\n\u001b[1;32m   1793\u001b[0m     \u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mGraphCompileReason(\u001b[39m\"\u001b[39;49m\u001b[39mreturn_value\u001b[39;49m\u001b[39m\"\u001b[39;49m, [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_summary()])\n\u001b[1;32m   1794\u001b[0m )\n\u001b[1;32m   1795\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39madd_output_instructions([create_instruction(\u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:541\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    538\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m count_calls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    540\u001b[0m     output\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 541\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, pass2\u001b[39m.\u001b[39;49mgraph_output_vars(), root)\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    545\u001b[0m         output\u001b[39m.\u001b[39mappend(pass2\u001b[39m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sdxl/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: _capture raised TVMError: Traceback (most recent call last):\n  [bt] (8) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::Normalize(tvm::RelayExpr const&)+0x113) [0x7f0f0fb229e3]\n  [bt] (7) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>::InitVTable()::{lambda(tvm::runtime::ObjectRef const&, tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>*)#9}::_FUN(tvm::runtime::ObjectRef const&, tvm::relax::ExprFunctor<tvm::RelayExpr (tvm::RelayExpr const&)>*)+0x2c) [0x7f0f0fb149ec]\n  [bt] (6) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::VisitExpr_(tvm::relax::CallNode const*)+0x563) [0x7f0f0fb25043]\n  [bt] (5) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::Normalizer::InferStructInfo(tvm::relax::Call const&)+0x2b2) [0x7f0f0fb24082]\n  [bt] (4) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::relax::StructInfo (tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>::AssignTypedLambda<tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)>(tvm::relax::StructInfo (*)(tvm::relax::Call const&, tvm::relax::BlockBuilder const&))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)+0x13c) [0x7f0f0fc25c6c]\n  [bt] (3) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::InferStructInfoSqueeze(tvm::relax::Call const&, tvm::relax::BlockBuilder const&)+0x441) [0x7f0f0fd51aa1]\n  [bt] (2) /home/guoyaol/tvm/build/libtvm.so(tvm::relax::BlockBuilderImpl::ReportFatal(tvm::Diagnostic const&)+0x59) [0x7f0f0fb1de59]\n  [bt] (1) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::detail::LogFatal::Entry::Finalize()+0x3d) [0x7f0f0f32668d]\n  [bt] (0) /home/guoyaol/tvm/build/libtvm.so(tvm::runtime::Backtrace[abi:cxx11]()+0x2c) [0x7f0f11bf169c]\n  File \"/home/guoyaol/tvm/src/relax/ir/block_builder.cc\", line 138\nTVMError: Squeeze expects the input tensor shape values at the given axis positions to be all 1. However, the tensor shape at axis 1 is T.int64(1280) which is not 1. If it is symbolic, please use MatchCast to cast it to 1 before doing Squeeze.\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "\n",
    "torch_dev_key = \"cpu\"\n",
    "\n",
    "clip = clip_to_text_embeddings(pipe)\n",
    "clip2 = clip_to_text_embeddings2(pipe)\n",
    "unet = unet_latents_to_noise_pred(pipe, torch_dev_key)\n",
    "vae = vae_to_image(pipe)\n",
    "concat_embeddings = concat_embeddings()\n",
    "concat_pool_embeddings = concat_pool_embeddings()\n",
    "concat_enocder_outputs = concat_enocder_outputs()\n",
    "image_to_rgba = image_to_rgba()\n",
    "scheduler_step = euler_discrete_scheduler_steps()\n",
    "scheduler_scale = euler_discrete_scheduler_scale()\n",
    "cat_latents = cat_latents()\n",
    "\n",
    "mod: tvm.IRModule = utils.merge_irmodules(\n",
    "    clip,\n",
    "    clip2,\n",
    "    unet,\n",
    "    cat_latents,\n",
    "    vae,\n",
    "    concat_embeddings,\n",
    "    concat_pool_embeddings,\n",
    "    concat_enocder_outputs,\n",
    "    image_to_rgba,\n",
    "    scheduler_step,\n",
    "    scheduler_scale,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relax.frontend.detach_params(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:46:23] /home/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[23:46:23] /home/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[23:46:36] /home/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n",
      "[23:46:36] /home/guoyaol/tvm/include/tvm/topi/transform.h:1126: Warning: Fast mode segfaults when there are out-of-bounds indices. Make sure input indices are in bound\n"
     ]
    }
   ],
   "source": [
    "mod = relax.pipeline.get_pipeline()(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"clip\", \"clip2\", \"unet\", \"vae\"]\n",
    "scheduler_func_names = [\"euler_discrete_scheduler_step\", \"euler_discrete_scheduler_scale\"]\n",
    "entry_funcs = (\n",
    "    model_names + scheduler_func_names  + [\"image_to_rgba\", \"concat_embeddings\", \"concat_enocder_outputs\", \"concat_pool_embeddings\", \"cat_latents\"]\n",
    ")\n",
    "\n",
    "# Clean up unused parts of the IRModule.\n",
    "mod = relax.transform.DeadCodeElimination(entry_funcs)(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = relax.transform.LiftTransformParams()(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_transform, mod_deploy = utils.split_transform_deploy_mod(\n",
    "    mod, model_names, entry_funcs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IRModule for build stage:\n",
      "vae_transform_params\n",
      "clip_transform_params\n",
      "unet_transform_params\n",
      "clip2_transform_params\n",
      "\n",
      "In IRModule for deployment stage:\n",
      "concat_pool_embeddings\n",
      "image_to_rgba\n",
      "concat_embeddings\n",
      "vae\n",
      "euler_discrete_scheduler_scale\n",
      "euler_discrete_scheduler_step\n",
      "concat_enocder_outputs\n",
      "cat_latents\n",
      "unet\n",
      "clip2\n",
      "clip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_relax_funcnames(mod: tvm.IRModule):\n",
    "    for global_var, func in mod.functions.items():\n",
    "        if isinstance(func, relax.Function):\n",
    "            print(global_var.name_hint)\n",
    "    print()\n",
    "    \n",
    "print(\"In IRModule for build stage:\")\n",
    "print_relax_funcnames(mod_transform)\n",
    "\n",
    "print(\"In IRModule for deployment stage:\")\n",
    "print_relax_funcnames(mod_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start storing to cache dist/params\n",
      "[2533/2533] saving clip2_516\n",
      "All finished, 231 total shards committed, record saved to dist/params/ndarray-cache.json\n",
      "Also saved a bf16 record to dist/params/ndarray-cache-b16.json\n"
     ]
    }
   ],
   "source": [
    "# Compute and save the scheduler constants.\n",
    "\n",
    "# trace.compute_save_scheduler_consts(artifact_path=\"dist\")\n",
    "#TODO: add this compute\n",
    "\n",
    "# Compute and save the models's weight parameters.\n",
    "new_params = utils.transform_params(mod_transform, params)\n",
    "utils.save_params(new_params, artifact_path=\"dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import meta_schedule as ms\n",
    "\n",
    "target = tvm.target.Target(\"cuda\")\n",
    "device = tvm.cuda()\n",
    "\n",
    "with target, tvm.transform.PassContext(opt_level=3):\n",
    "    mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = relax.build(mod=mod_deploy, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.export_library(\"dist/stable_diffusion.so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weight parameters back.\n",
    "target = tvm.target.Target(\"cuda\")\n",
    "device = tvm.cuda()\n",
    "const_params_dict = utils.load_params(artifact_path=\"dist\", device=device)\n",
    "# Load the model executable back from the shared library.\n",
    "ex = tvm.runtime.load_module(\"dist/stable_diffusion.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = relax.VirtualMachine(rt_mod=ex, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(f, params):\n",
    "    def wrapped_f(*args):\n",
    "        return f(*args, params)\n",
    "\n",
    "    return wrapped_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from web_stable_diffusion import runtime\n",
    "\n",
    "\n",
    "class EulerDiscreteScheduler(runtime.Scheduler):\n",
    "    scheduler_name = \"euler-discrete-solver\"\n",
    "\n",
    "    def __init__(self, artifact_path: str, device) -> None:\n",
    "        with open(\n",
    "            f\"{artifact_path}/scheduler_euler_discrete_consts.json\", \"r\"\n",
    "        ) as file:\n",
    "            jsoncontent = file.read()\n",
    "        scheduler_consts = json.loads(jsoncontent)\n",
    "\n",
    "        def f_convert(data, dtype):\n",
    "            return [tvm.nd.array(np.array(t, dtype=dtype), device) for t in data]\n",
    "\n",
    "        self.timesteps = f_convert(scheduler_consts[\"timesteps\"], \"int32\")\n",
    "        self.sigma = f_convert(scheduler_consts[\"sigma\"], \"float32\")\n",
    "\n",
    "        # self.last_model_output: tvm.nd.NDArray = tvm.nd.empty(\n",
    "        #     (1, 4, 64, 64), \"float32\", device\n",
    "        # )\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        vm: relax.VirtualMachine,\n",
    "        model_output: tvm.nd.NDArray,\n",
    "        sample: tvm.nd.NDArray,\n",
    "        counter: int,\n",
    "    ) -> tvm.nd.NDArray:\n",
    "        # model_output = vm[\"dpm_solver_multistep_scheduler_convert_model_output\"](\n",
    "        #     sample, model_output, self.alpha[counter], self.sigma[counter]\n",
    "        # )\n",
    "        prev_latents = vm[\"euler_discrete_scheduler_step\"](\n",
    "            sample,\n",
    "            model_output,\n",
    "            self.sigma[counter],\n",
    "            self.sigma[counter+1]\n",
    "        )\n",
    "        # self.last_model_output = model_output\n",
    "        return prev_latents\n",
    "    \n",
    "    def scale_model_input(self, vm, sample: tvm.nd.NDArray, counter) -> tvm.nd.NDArray:\n",
    "        result = vm[\"euler_discrete_scheduler_scale\"](sample, self.sigma[counter])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion XL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "\n",
    "class TVMSDPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vm: relax.VirtualMachine,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        tokenizer2: CLIPTokenizer,\n",
    "        scheduler: runtime.Scheduler,\n",
    "        tvm_device,\n",
    "        param_dict,\n",
    "    ):\n",
    "        def wrapper(f, params):\n",
    "            def wrapped_f(*args):\n",
    "                return f(*args, params)\n",
    "\n",
    "            return wrapped_f\n",
    "\n",
    "        self.vm = vm\n",
    "        self.clip_to_text_embeddings = wrapper(vm[\"clip\"], param_dict[\"clip\"])\n",
    "        self.clip_to_text_embeddings2 = wrapper(vm[\"clip2\"], param_dict[\"clip2\"])\n",
    "        self.unet_latents_to_noise_pred = wrapper(vm[\"unet\"], param_dict[\"unet\"])\n",
    "        self.vae_to_image = wrapper(vm[\"vae\"], param_dict[\"vae\"])\n",
    "        self.concat_embeddings = vm[\"concat_embeddings\"]\n",
    "        self.concat_enocder_outputs = vm[\"concat_enocder_outputs\"]\n",
    "        self.concat_pool_embeddings = vm[\"concat_pool_embeddings\"]\n",
    "        self.image_to_rgba = vm[\"image_to_rgba\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer2 = tokenizer2\n",
    "        self.scheduler = scheduler\n",
    "        self.tvm_device = tvm_device\n",
    "        self.param_dict = param_dict\n",
    "\n",
    "    def __call__(self, prompt: str, negative_prompt: str = \"\"):\n",
    "        # The height and width are fixed to 512.\n",
    "\n",
    "        # Compute the embeddings for the prompt and negative prompt.\n",
    "        list_text_embeddings = []\n",
    "\n",
    "        tokenizers = [self.tokenizer, self.tokenizer2]\n",
    "        text_encoders = [self.clip_to_text_embeddings, self.clip_to_text_embeddings2]\n",
    "\n",
    "        #prompt\n",
    "        for tokenizer, text_encoder in zip(tokenizers, text_encoders):\n",
    "            text_inputs = tokenizer(\n",
    "                    prompt,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=tokenizer.model_max_length,\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "            text_input_ids = text_inputs.input_ids.to(torch.int32)\n",
    "            # Clip the text if the length exceeds the maximum allowed length.\n",
    "            if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
    "                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n",
    "\n",
    "            # Compute text embeddings.\n",
    "            text_input_ids = tvm.nd.array(text_input_ids.cpu().numpy(), self.tvm_device)\n",
    "            clip_output = text_encoder(text_input_ids)\n",
    "            text_embeddings = clip_output[0]\n",
    "            pooled_prompt_embeds = clip_output[1]\n",
    "\n",
    "        \n",
    "        #negative prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # for text in [negative_prompt, prompt]:\n",
    "        #     text = [text]\n",
    "        #     # Tokenize the text.\n",
    "        #     text_inputs = self.tokenizer(\n",
    "        #         text,\n",
    "        #         padding=\"max_length\",\n",
    "        #         max_length=self.tokenizer.model_max_length,  # 77\n",
    "        #         return_tensors=\"pt\",\n",
    "        #     )\n",
    "        #     text_input_ids = text_inputs.input_ids.to(torch.int32)\n",
    "        #     # Clip the text if the length exceeds the maximum allowed length.\n",
    "        #     if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
    "        #         text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n",
    "\n",
    "        #     # Compute text embeddings.\n",
    "        #     text_input_ids = tvm.nd.array(text_input_ids.cpu().numpy(), self.tvm_device)\n",
    "        #     clip_output = self.clip_to_text_embeddings(text_input_ids)\n",
    "        #     text_embeddings = clip_output[0]\n",
    "        #     pooled_prompt_embeds = clip_output[1]\n",
    "\n",
    "        #     list_text_embeddings.append(text_embeddings)\n",
    "\n",
    "        # #TODO convert data to tvm.nd.array, fold into TVM\n",
    "        # torch_template = torch.from_numpy(pooled_prompt_embeds.asnumpy())\n",
    "        # negative_pooled_prompt_embeds = torch.zeros_like(torch_template)\n",
    "        # negative_pooled_prompt_embeds = tvm.nd.array(negative_pooled_prompt_embeds, self.tvm_device)\n",
    "        # pooled_list_text_embeddings = [negative_pooled_prompt_embeds, pooled_prompt_embeds]\n",
    "\n",
    "        \n",
    "        # # Concatenate the text embeddings.\n",
    "        # text_embeddings = self.concat_embeddings(*list_text_embeddings)\n",
    "\n",
    "        # add_text_embeds = self.concat_embeddings(*pooled_list_text_embeddings)\n",
    "        # print(add_text_embeds.shape)\n",
    "\n",
    "        #TODO: check correct, fold into TVM\n",
    "        add_time_ids = torch.tensor([[1024., 1024., 0., 0., 1024., 1024.],[1024., 1024., 0., 0., 1024., 1024.]], dtype=torch.float32)\n",
    "        add_time_ids = tvm.nd.array(add_time_ids, self.tvm_device)\n",
    "\n",
    "\n",
    "        # Randomly initialize the latents.\n",
    "        latents = torch.randn(\n",
    "            (1, 4, 128, 128),\n",
    "            device=\"cpu\",\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        latents = tvm.nd.array(latents.numpy(), self.tvm_device)\n",
    "\n",
    "        # UNet iteration.\n",
    "        for i in tqdm(range(len(self.scheduler.timesteps))):\n",
    "            #TODO: add this\n",
    "            #latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "            t = self.scheduler.timesteps[i]\n",
    "            noise_pred = self.unet_latents_to_noise_pred(latents, t, text_embeddings, add_text_embeds, add_time_ids)\n",
    "            latents = self.scheduler.step(self.vm, noise_pred, latents, i)\n",
    "\n",
    "        # VAE decode.\n",
    "        image = self.vae_to_image(latents)\n",
    "\n",
    "        # Transform generated image to RGBA mode.\n",
    "        image = self.image_to_rgba(image)\n",
    "        return Image.fromarray(image.numpy().view(\"uint8\").reshape(1024, 1024, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TVMSDPipeline(\n",
    "    vm=vm,\n",
    "    tokenizer=CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    "    tokenizer2=CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    "    scheduler=runtime.EulerDiscreteScheduler(artifact_path=\"dist\", device=device),\n",
    "    tvm_device=device,\n",
    "    param_dict=const_params_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"Jellyfish floating in a forest\"\n",
    "\n",
    "start = time.time()\n",
    "image = pipe(prompt)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time elapsed: {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
